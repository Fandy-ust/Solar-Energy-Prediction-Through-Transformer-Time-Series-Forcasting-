import torch
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from tqdm import tqdm
import numpy as np
import os
import pandas as pd
import json
import math
import warnings
import torch.nn as nn
import torch.nn.functional as F

ALL_TRAIN_DATA_PATH = r"C:\Users\1\Documents\Solarpower\Data_processed\Train_val_test_split_new\train_selected_sites.csv"
ALL_VAL_DATA_PATH = r"C:\Users\1\Documents\Solarpower\Data_processed\Train_val_test_split_new\validation_enhanced.csv"
STATS_FILE_PATH = r"C:\Users\1\Documents\Solarpower\Data_processed\Train_val_test_split_new\scaler\campus_scale.txt"


PHASE1_MODEL_PATH = r"C:\Python\Trained_models\Elite_Model_Transformer_Phase1_Complete\transformer_elite_model_phase1.pth"

PHASE2A_SAVE_DIR = r"C:\Python\Trained_models\Elite_Model_Transformer_Phase2A_Complete"
os.makedirs(PHASE2A_SAVE_DIR, exist_ok=True)
PHASE2A_MODEL_SAVE_PATH = os.path.join(PHASE2A_SAVE_DIR, 'transformer_elite_model_phase2a.pth')

TARGET_COLUMN = 'SolarGeneration'
WEATHER_FEATURES = ['AirTemperature', 'Ghi_hourly', 'CloudOpacity_hourly', "minutes_since_last_update",
                    "RelativeHumidity"]
TIME_FEATURES = ["zenith_sin", "azimuth_sin", "day_sin", "year_sin"]
ENCODER_FEATURES = WEATHER_FEATURES + TIME_FEATURES + [TARGET_COLUMN]
FUTURE_KNOWN_FEATURES = TIME_FEATURES
DECODER_INPUT_FEATURES = FUTURE_KNOWN_FEATURES + [TARGET_COLUMN]
TARGET_FEATURE_INDEX = DECODER_INPUT_FEATURES.index(TARGET_COLUMN)

LOOKBACK_STEPS = 4 * 24
LOOKFORWARD_STEPS = 4 * 5
BATCH_SIZE = 32
LEARNING_RATE_PHASE2A = 0.5e-6
EPOCHS_PHASE2A = 5 
MAX_GRAD_NORM = 1.0
PATIENCE = 3
SENTINEL_VALUE = -999.0
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

D_MODEL = 128
NHEAD = 8
NUM_ENCODER_LAYERS = 3
NUM_DECODER_LAYERS = 3
DIM_FEEDFORWARD = 512
DROPOUT_RATE = 0.1


class PositionalEncoding(nn.Module):
    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):
        super().__init__()
        self.dropout = nn.Dropout(p=dropout)
        position = torch.arange(max_len).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))
        pe = torch.zeros(1, max_len, d_model)
        pe[0, :, 0::2] = torch.sin(position * div_term)
        pe[0, :, 1::2] = torch.cos(position * div_term)
        self.register_buffer('pe', pe)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = x + self.pe[:, :x.size(1)]
        return self.dropout(x)


class TimeSeriesTransformer(nn.Module):
    def __init__(self, encoder_feature_dim: int, decoder_feature_dim: int, d_model: int, nhead: int,
                 num_encoder_layers: int, num_decoder_layers: int, dim_feedforward: int, dropout: float):
        super().__init__()
        self.d_model = d_model
        self.encoder_embedding = nn.Linear(encoder_feature_dim, d_model)
        self.decoder_embedding = nn.Linear(decoder_feature_dim, d_model)
        self.pos_encoder = PositionalEncoding(d_model, dropout)
        self.transformer = nn.Transformer(
            d_model=d_model, nhead=nhead, num_encoder_layers=num_encoder_layers,
            num_decoder_layers=num_decoder_layers, dim_feedforward=dim_feedforward,
            dropout=dropout, batch_first=True
        )
        self.output_layer = nn.Linear(d_model, 1)

    def _generate_square_subsequent_mask(self, sz: int) -> torch.Tensor:
        return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)

    def forward(self, src: torch.Tensor, tgt: torch.Tensor,
                tgt_key_padding_mask: torch.Tensor,
                src_key_padding_mask: torch.Tensor) -> torch.Tensor:
        tgt_mask = self._generate_square_subsequent_mask(tgt.size(1)).to(src.device)

        src_emb = self.encoder_embedding(src) * math.sqrt(self.d_model)
        src_emb = self.pos_encoder(src_emb)

        tgt_emb = self.decoder_embedding(tgt) * math.sqrt(self.d_model)
        tgt_emb = self.pos_encoder(tgt_emb)

        memory = self.transformer.encoder(
            src=src_emb,
            src_key_padding_mask=src_key_padding_mask
        )

        decoder_output = self.transformer.decoder(
            tgt=tgt_emb,
            memory=memory,
            tgt_mask=tgt_mask,
            tgt_key_padding_mask=tgt_key_padding_mask,
            memory_key_padding_mask=src_key_padding_mask,
        )
        return self.output_layer(decoder_output)

class SolarPowerDataset_Transformer(Dataset):
    def __init__(self, df, lookback, lookforward):
        
        self.df = df
        self.lookback = lookback
        self.lookforward = lookforward
        self.total_seq_len = lookback + lookforward
        self.campus_indices = []
        for campus_id in df['CampusKey'].unique():
            campus_df = df[df['CampusKey'] == campus_id]
            start_index = campus_df.index[0]
            num_samples = len(campus_df) - self.total_seq_len + 1
            if num_samples > 0:
                self.campus_indices.append({'start': start_index, 'count': num_samples})
        self.total_samples = sum(c['count'] for c in self.campus_indices)

    def __len__(self):
        return self.total_samples

    def __getitem__(self, idx):
        campus_idx = 0
        while idx >= self.campus_indices[campus_idx]['count']:
            idx -= self.campus_indices[campus_idx]['count']
            campus_idx += 1

        start_pos = self.campus_indices[campus_idx]['start'] + idx
        end_pos = start_pos + self.total_seq_len
        sequence_slice = self.df.iloc[start_pos:end_pos]

        src_df = sequence_slice.iloc[:self.lookback].copy()  
   
        src_key_padding_mask = torch.tensor(src_df[TARGET_COLUMN].isna().values, dtype=torch.bool)

        src_df.fillna(0, inplace=True)
        src = torch.tensor(src_df[ENCODER_FEATURES].values, dtype=torch.float32)

        future_slice = sequence_slice.iloc[self.lookback:]

        tgt_key_padding_mask = torch.tensor(future_slice[TARGET_COLUMN].isna().values, dtype=torch.bool)

        last_obs = sequence_slice.iloc[self.lookback - 1][TARGET_COLUMN]

        shifted = future_slice[TARGET_COLUMN].shift(1)

        shifted.iloc[0] = last_obs
        shifted = shifted.fillna(0)
        future_known_df = future_slice[FUTURE_KNOWN_FEATURES]
        tgt_input_df = pd.concat([future_known_df.reset_index(drop=True),
                                  shifted.reset_index(drop=True).rename(TARGET_COLUMN)], axis=1)
        tgt_input = torch.tensor(tgt_input_df[DECODER_INPUT_FEATURES].values, dtype=torch.float32)

        tgt_output_series = future_slice[TARGET_COLUMN].fillna(SENTINEL_VALUE)
        tgt_output = torch.tensor(tgt_output_series.values, dtype=torch.float32).unsqueeze(-1)

        return src, tgt_input, tgt_output, tgt_key_padding_mask, src_key_padding_mask


def masked_mse_loss(predictions: torch.Tensor, targets: torch.Tensor, sentinel_value: float = SENTINEL_VALUE):
    mask = targets != sentinel_value
    if not mask.any():
        return torch.tensor(0.0, device=predictions.device, requires_grad=True)
    predictions_masked = torch.masked_select(predictions, mask)
    targets_masked = torch.masked_select(targets, mask)
    return F.mse_loss(predictions_masked, targets_masked)


def apply_zscore_scaling(df_data, columns, stats):
    df_scaled = df_data.copy()
    for col in columns:
        if col in stats and stats[col]['std'] > 1e-6:
            df_scaled[col] = (df_scaled[col] - stats[col]['mean']) / stats[col]['std']
    return df_scaled


def scale_dataframe_by_campus(df, scaler_stats):
    if df.empty:
        return pd.DataFrame()

    scaled_dfs = []
    columns_to_scale = WEATHER_FEATURES + [TARGET_COLUMN]

    for campus_id_num, group in df.groupby('CampusKey'):
        group_copy = group.copy()
        campus_id = str(campus_id_num)
        campus_params = scaler_stats.get(campus_id)

        if campus_params:
            for col in WEATHER_FEATURES:
                if col in group_copy.columns:
                    group_copy[col] = group_copy[col].fillna(0)

            scaled_group = apply_zscore_scaling(group_copy, columns_to_scale, campus_params)
            scaled_dfs.append(scaled_group)

    if not scaled_dfs:
        return pd.DataFrame()

    return pd.concat(scaled_dfs).sort_index()

def train_epoch_phase2a(model, data_loader, optimizer, criterion, device):

    model.train()
    total_loss_for_logging = 0
    num_batches_processed = 0

    progress_bar = tqdm(data_loader, desc=f"Phase 2A Training (Step-wise Supervision)", leave=True, dynamic_ncols=True)

    for batch in progress_bar:
        src, tgt_input, tgt_output, tgt_padding_mask, src_padding_mask = \
            batch[0].to(device), batch[1].to(device), batch[2].to(device), \
                batch[3].to(device), batch[4].to(device)

        optimizer.zero_grad()

        nan_mask = (tgt_output[:, :, 0] == SENTINEL_VALUE) 
        max_nans_in_batch = torch.max(torch.sum(nan_mask, dim=1)).item()

        if max_nans_in_batch == 0:
            outputs = model(src, tgt_input, tgt_padding_mask, src_padding_mask)
            loss = criterion(outputs, tgt_output)
            if not torch.isnan(loss) and loss.item() > 0:
                loss.backward()
                total_loss_for_logging += loss.item()
        else:
            working_tgt_input = tgt_input.clone()
            nan_cumsum = torch.cumsum(nan_mask.int(), dim=1)

            for k in range(int(max_nans_in_batch)):
                current_model_output = model(src, working_tgt_input, tgt_padding_mask, src_padding_mask)

                loss_k = criterion(current_model_output, tgt_output)

                if not torch.isnan(loss_k) and loss_k.item() > 0:
                    weight = (k + 1) / max_nans_in_batch
                    (loss_k * weight).backward()

                    if k == max_nans_in_batch - 1: 
                        total_loss_for_logging += loss_k.item()

                if k == max_nans_in_batch - 1:
                    break

                is_kth_nan = (nan_cumsum == k + 1) & nan_mask

                predictions_for_kth_nan = current_model_output.detach()[:, :, 0][is_kth_nan]

                working_tgt_input[:, :, TARGET_FEATURE_INDEX][is_kth_nan] = predictions_for_kth_nan

        torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)
        optimizer.step()

        num_batches_processed += 1
        if num_batches_processed > 0:
            progress_bar.set_postfix(avg_loss=total_loss_for_logging / num_batches_processed)

    return total_loss_for_logging / num_batches_processed if num_batches_processed > 0 else 0


def validate_epoch_phase2a(model, data_loader, criterion, device):
    model.eval()
    total_val_loss = 0

    progress_bar = tqdm(data_loader, desc=f"Phase 2A Validation", leave=False, dynamic_ncols=True)
    with torch.no_grad():
        for batch in progress_bar:
            src, tgt_input, tgt_output, tgt_padding_mask, src_padding_mask = \
                batch[0].to(device), batch[1].to(device), batch[2].to(device), \
                    batch[3].to(device), batch[4].to(device)

            nan_mask = (tgt_output[:, :, 0] == SENTINEL_VALUE)
            max_nans_in_batch = torch.max(torch.sum(nan_mask, dim=1)).item()

            final_output = None
            if max_nans_in_batch == 0:
                final_output = model(src, tgt_input, tgt_padding_mask, src_padding_mask)
            else:
                working_tgt_input = tgt_input.clone()
                nan_cumsum = torch.cumsum(nan_mask.int(), dim=1)
                for k in range(int(max_nans_in_batch)):
                    current_model_output = model(src, working_tgt_input, tgt_padding_mask, src_padding_mask)
                    if k == max_nans_in_batch - 1:
                        final_output = current_model_output
                        break

                    is_kth_nan = (nan_cumsum == k + 1) & nan_mask
                    predictions_for_kth_nan = current_model_output[:, :, 0][is_kth_nan]
                    working_tgt_input[:, :, TARGET_FEATURE_INDEX][is_kth_nan] = predictions_for_kth_nan

            loss = criterion(final_output, tgt_output)
            if not torch.isnan(loss):
                total_val_loss += loss.item()

    return total_val_loss / len(data_loader)



if __name__ == "__main__":

    df_train_all = pd.read_csv(ALL_TRAIN_DATA_PATH)
    df_val_all = pd.read_csv(ALL_VAL_DATA_PATH)
    with open(STATS_FILE_PATH, 'r') as f:
        scaling_stats = json.load(f)

    df_train_scaled = scale_dataframe_by_campus(df_train_all, scaling_stats)
    df_val_scaled = scale_dataframe_by_campus(df_val_all, scaling_stats)

    train_dataset = SolarPowerDataset_Transformer(df_train_scaled, LOOKBACK_STEPS, LOOKFORWARD_STEPS)
    val_dataset = SolarPowerDataset_Transformer(df_val_scaled, LOOKBACK_STEPS, LOOKFORWARD_STEPS)
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)

    model = TimeSeriesTransformer(
        encoder_feature_dim=len(ENCODER_FEATURES),
        decoder_feature_dim=len(DECODER_INPUT_FEATURES),
        d_model=D_MODEL, nhead=NHEAD, num_encoder_layers=NUM_ENCODER_LAYERS,
        num_decoder_layers=NUM_DECODER_LAYERS, dim_feedforward=DIM_FEEDFORWARD,
        dropout=DROPOUT_RATE
    ).to(DEVICE)

    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE_PHASE2A)
    criterion = masked_mse_loss

    best_val_loss = float('inf')
    patience_counter = 0

    for epoch in range(EPOCHS_PHASE2A):
        print(f"\nEpoch {epoch + 1}/{EPOCHS_PHASE2A}")

        train_loss = train_epoch_phase2a(model, train_loader, optimizer, criterion, DEVICE)
        val_loss = validate_epoch_phase2a(model, val_loader, criterion, DEVICE)

        print(f"    Epoch {epoch + 1}, Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}")

        if val_loss < best_val_loss:
            best_val_loss = val_loss
            torch.save(model.state_dict(), PHASE2A_MODEL_SAVE_PATH)
            patience_counter = 0
            print(f"      -> New best validation loss. Model saved to {PHASE2A_MODEL_SAVE_PATH}")
        else:
            patience_counter += 1
            if patience_counter >= PATIENCE:
                break

    print(f"\nPhase2A training conplete. Best Validaation Loss: {best_val_loss:.6f}")
    print(f"Model saved at: {PHASE2A_MODEL_SAVE_PATH}")
