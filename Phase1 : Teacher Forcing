import os
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import torch.optim as optim
from tqdm import tqdm
import json
import warnings
import math
import matplotlib.pyplot as plt


ALL_TRAIN_DATA_PATH = r"C:\Users\1\Documents\Solarpower\Data_processed\Train_val_test_split_new\train_selected_sites.csv"
ALL_VAL_DATA_PATH = r"C:\Users\1\Documents\Solarpower\Data_processed\Train_val_test_split_new\validation_enhanced.csv"
STATS_FILE_PATH = r"C:\Users\1\Documents\Solarpower\Data_processed\Train_val_test_split_new\scaler\campus_scale.txt"
MODEL_SAVE_DIR = r"C:\Python\Trained_models\Elite_Model_Transformer_Phase1_Complete" 
os.makedirs(MODEL_SAVE_DIR, exist_ok=True)


TARGET_COLUMN = 'SolarGeneration'
WEATHER_FEATURES = ['AirTemperature', 'Ghi_hourly', 'CloudOpacity_hourly',"minutes_since_last_update", "RelativeHumidity"]
TIME_FEATURES = ["zenith_sin","azimuth_sin","day_sin","year_sin"]
ENCODER_FEATURES = WEATHER_FEATURES + TIME_FEATURES + [TARGET_COLUMN]
FUTURE_KNOWN_FEATURES = TIME_FEATURES
DECODER_INPUT_FEATURES = FUTURE_KNOWN_FEATURES + [TARGET_COLUMN]


LOOKBACK_STEPS = 4 * 24
LOOKFORWARD_STEPS = 4 * 5
BATCH_SIZE = 32
EPOCHS = 100
LEARNING_RATE = 0.0001
MAX_GRAD_NORM = 1.0
PATIENCE = 5
SENTINEL_VALUE = -999.0


D_MODEL = 128
NHEAD = 8
NUM_ENCODER_LAYERS = 3
NUM_DECODER_LAYERS = 3
DIM_FEEDFORWARD = 512
DROPOUT_RATE = 0.1

class PositionalEncoding(nn.Module):
    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):
        super().__init__()
        self.dropout = nn.Dropout(p=dropout)
        position = torch.arange(max_len).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))
        pe = torch.zeros(1, max_len, d_model)
        pe[0, :, 0::2] = torch.sin(position * div_term)
        pe[0, :, 1::2] = torch.cos(position * div_term)
        self.register_buffer('pe', pe)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = x + self.pe[:, :x.size(1)]
        return self.dropout(x)


class TimeSeriesTransformer(nn.Module):
    def __init__(self, encoder_feature_dim: int, decoder_feature_dim: int, d_model: int, nhead: int,
                 num_encoder_layers: int, num_decoder_layers: int, dim_feedforward: int, dropout: float):
        super().__init__()
        self.d_model = d_model
        self.encoder_embedding = nn.Linear(encoder_feature_dim, d_model)
        self.decoder_embedding = nn.Linear(decoder_feature_dim, d_model)
        self.pos_encoder = PositionalEncoding(d_model, dropout)
        self.transformer = nn.Transformer(
            d_model=d_model, nhead=nhead, num_encoder_layers=num_encoder_layers,
            num_decoder_layers=num_decoder_layers, dim_feedforward=dim_feedforward,
            dropout=dropout, batch_first=True
        )
        self.output_layer = nn.Linear(d_model, 1)

    def _generate_square_subsequent_mask(self, sz: int) -> torch.Tensor:
        return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)

    def forward(self, src: torch.Tensor, tgt: torch.Tensor,
                tgt_key_padding_mask: torch.Tensor,
                src_key_padding_mask: torch.Tensor) -> torch.Tensor:
        tgt_mask = self._generate_square_subsequent_mask(tgt.size(1)).to(src.device)

        src_emb = self.encoder_embedding(src) * math.sqrt(self.d_model)
        src_emb = self.pos_encoder(src_emb)

        tgt_emb = self.decoder_embedding(tgt) * math.sqrt(self.d_model)
        tgt_emb = self.pos_encoder(tgt_emb)

        memory = self.transformer.encoder(
            src=src_emb,
            src_key_padding_mask=src_key_padding_mask
        )

        decoder_output = self.transformer.decoder(
            tgt=tgt_emb,
            memory=memory,
            tgt_mask=tgt_mask,
            tgt_key_padding_mask=tgt_key_padding_mask,
            memory_key_padding_mask=src_key_padding_mask,
        )
        return self.output_layer(decoder_output)

class SolarPowerDataset_Transformer(Dataset):
    def __init__(self, df, lookback, lookforward):
        self.df = df
        self.lookback = lookback
        self.lookforward = lookforward
        self.total_seq_len = lookback + lookforward
        self.campus_indices = []
        for campus_id in df['CampusKey'].unique():
            campus_df = df[df['CampusKey'] == campus_id]
            start_index = campus_df.index[0]
            num_samples = len(campus_df) - self.total_seq_len + 1
            if num_samples > 0:
                self.campus_indices.append({'start': start_index, 'count': num_samples})
        self.total_samples = sum(c['count'] for c in self.campus_indices)

    def __len__(self):
        return self.total_samples
    def __getitem__(self, idx):
        campus_idx = 0
        while idx >= self.campus_indices[campus_idx]['count']:
            idx -= self.campus_indices[campus_idx]['count']
            campus_idx += 1

        start_pos = self.campus_indices[campus_idx]['start'] + idx
        end_pos = start_pos + self.total_seq_len
        sequence_slice = self.df.iloc[start_pos:end_pos]

        src_df = sequence_slice.iloc[:self.lookback].copy()  

        src_key_padding_mask = torch.tensor(src_df[TARGET_COLUMN].isna().values, dtype=torch.bool)

        src_df.fillna(0, inplace=True)
        src = torch.tensor(src_df[ENCODER_FEATURES].values, dtype=torch.float32)

        future_slice = sequence_slice.iloc[self.lookback:]

        tgt_key_padding_mask = torch.tensor(future_slice[TARGET_COLUMN].isna().values, dtype=torch.bool)

        last_obs = sequence_slice.iloc[self.lookback - 1][TARGET_COLUMN]

        shifted = future_slice[TARGET_COLUMN].shift(1)

        shifted.iloc[0] = last_obs
        shifted = shifted.fillna(0)
        future_known_df = future_slice[FUTURE_KNOWN_FEATURES]
        tgt_input_df = pd.concat([future_known_df.reset_index(drop=True),
                                  shifted.reset_index(drop=True).rename(TARGET_COLUMN)], axis=1)
        tgt_input = torch.tensor(tgt_input_df[DECODER_INPUT_FEATURES].values, dtype=torch.float32)

        tgt_output_series = future_slice[TARGET_COLUMN].fillna(SENTINEL_VALUE)
        tgt_output = torch.tensor(tgt_output_series.values, dtype=torch.float32).unsqueeze(-1)

        return src, tgt_input, tgt_output, tgt_key_padding_mask, src_key_padding_mask

def masked_mse_loss(predictions: torch.Tensor, targets: torch.Tensor, sentinel_value: float = SENTINEL_VALUE):
    mask = targets != sentinel_value
    if not mask.any():
        return torch.tensor(0.0, device=predictions.device, requires_grad=True)
    predictions_masked = torch.masked_select(predictions, mask)
    targets_masked = torch.masked_select(targets, mask)
    return F.mse_loss(predictions_masked, targets_masked)

def apply_zscore_scaling(df_data, columns, stats):
    df_scaled = df_data.copy()
    for col in columns:
        if col in stats and stats[col]['std'] > 1e-6:
            df_scaled[col] = (df_scaled[col] - stats[col]['mean']) / stats[col]['std']
    return df_scaled

def scale_dataframe_by_campus(df, scaler_stats):
    if df.empty:
        return pd.DataFrame()

    scaled_dfs = []
    columns_to_scale = WEATHER_FEATURES + [TARGET_COLUMN]

    for campus_id_num, group in df.groupby('CampusKey'):
        group_copy = group.copy()
        campus_id = str(campus_id_num)
        campus_params = scaler_stats.get(campus_id)

        if campus_params:
            for col in WEATHER_FEATURES:
                if col in group_copy.columns:
                    group_copy[col] = group_copy[col].fillna(0)

            scaled_group = apply_zscore_scaling(group_copy, columns_to_scale, campus_params)
            scaled_dfs.append(scaled_group)

    if not scaled_dfs:
        return pd.DataFrame()

    return pd.concat(scaled_dfs).sort_index()


if __name__ == "__main__":
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
   
    df_train_all = pd.read_csv(ALL_TRAIN_DATA_PATH)
    df_val_all = pd.read_csv(ALL_VAL_DATA_PATH)
    with open(STATS_FILE_PATH, 'r') as f:
        scaling_stats = json.load(f)

    df_val_global = df_val_all.copy()
    df_train_scaled = scale_dataframe_by_campus(df_train_all, scaling_stats)
    df_val_scaled = scale_dataframe_by_campus(df_val_global, scaling_stats)

    if df_train_scaled.empty or df_val_scaled.empty:
        exit(1)

    train_dataset = SolarPowerDataset_Transformer(df_train_scaled, LOOKBACK_STEPS, LOOKFORWARD_STEPS)
    val_dataset = SolarPowerDataset_Transformer(df_val_scaled, LOOKBACK_STEPS, LOOKFORWARD_STEPS)
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)

    model = TimeSeriesTransformer(
        encoder_feature_dim=len(ENCODER_FEATURES),
        decoder_feature_dim=len(DECODER_INPUT_FEATURES),
        d_model=D_MODEL, nhead=NHEAD, num_encoder_layers=NUM_ENCODER_LAYERS,
        num_decoder_layers=NUM_DECODER_LAYERS, dim_feedforward=DIM_FEEDFORWARD,
        dropout=DROPOUT_RATE
    ).to(device)
    criterion = masked_mse_loss
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)

    best_val_loss = float('inf')
    patience_counter = 0
    train_losses, val_losses = [], []
    model_save_path = os.path.join(MODEL_SAVE_DIR, 'transformer_elite_model_phase1_V3.pth')

    for epoch in range(EPOCHS):
        model.train()
        total_train_loss = 0
        for src, tgt_input, tgt_output, tgt_padding_mask, src_padding_mask in tqdm(train_loader,
                                                                                   desc=f"Epoch {epoch + 1}/{EPOCHS} Train",
                                                                                   leave=False):
            src, tgt_input, tgt_output, tgt_padding_mask, src_padding_mask = \
                src.to(device), tgt_input.to(device), tgt_output.to(device), \
                    tgt_padding_mask.to(device), src_padding_mask.to(device)

            optimizer.zero_grad()
            outputs = model(src, tgt_input, tgt_padding_mask, src_padding_mask)

            loss = criterion(outputs, tgt_output)
            if torch.isnan(loss):
                continue
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)
            optimizer.step()
            total_train_loss += loss.item()

        epoch_train_loss = total_train_loss / len(train_loader)
        train_losses.append(epoch_train_loss)

        model.eval()
        total_val_loss = 0
        with torch.no_grad():
            for src, tgt_input, tgt_output, tgt_padding_mask, src_padding_mask in val_loader:
                src, tgt_input, tgt_output, tgt_padding_mask, src_padding_mask = \
                    src.to(device), tgt_input.to(device), tgt_output.to(device), \
                        tgt_padding_mask.to(device), src_padding_mask.to(device)

                outputs = model(src, tgt_input, tgt_padding_mask, src_padding_mask)
                loss = criterion(outputs, tgt_output)
        epoch_val_loss = total_val_loss / len(val_loader)
        val_losses.append(epoch_val_loss)

        print(f"    Epoch {epoch + 1}/{EPOCHS}, Train Loss: {epoch_train_loss:.6f}, Val Loss: {epoch_val_loss:.6f}")

        if epoch_val_loss < best_val_loss:
            best_val_loss = epoch_val_loss
            torch.save(model.state_dict(), model_save_path)
            patience_counter = 0
            print(f"      -> New best validation loss. Model saved to {model_save_path}")
        else:
            patience_counter += 1
            if patience_counter >= PATIENCE:
                break

    print(f"\Best Validation Loss: {best_val_loss:.6f}")

    plt.figure(figsize=(12, 7))
    plt.plot(train_losses, label='Training Loss (on 5 Champions)')
    plt.plot(val_losses, label='Validation Loss (on all other sites)')
    plt.title('Complete Transformer Elite Model Training History (Phase 1)')
    plt.xlabel('Epoch')
    plt.ylabel('Masked MSE Loss')
    plt.legend()
    plt.grid(True)
    plt.savefig(os.path.join(MODEL_SAVE_DIR, 'transformer_elite_model_training_history_complete.png'))
    plt.show()

    print(f"\n{'=' * 30} Transformer phase1 training completed {'=' * 30}")
    print(f"Model saved at : {MODEL_SAVE_DIR}")
